import re
import time
import signal
import subprocess

import ollama
import openai
from openai import OpenAI
# from ollama import chat
# from ollama import ChatResponse
# from ollama import Client
import tiktoken

from config import OPENSOURCE_MODELS
from logger import setup_logger

logger = setup_logger('llm_client', 'logs/llm_client.log')

IMPORT_DWSIM = (
    'dwsimpath = "C:\\Users\\jcvid\\AppData\\Local\\DWSIM\\" \n'
    'clr.AddReference("System")\n'
    'clr.AddReference(dwsimpath + "CapeOpen.dll")\n'
    'clr.AddReference(dwsimpath + "DWSIM.Automation.dll")\n'
    'clr.AddReference(dwsimpath + "DWSIM.Interfaces.dll")\n'
    'clr.AddReference(dwsimpath + "DWSIM.GlobalSettings.dll")\n'
    'clr.AddReference(dwsimpath + "DWSIM.SharedClasses.dll")\n'
    'clr.AddReference(dwsimpath + "DWSIM.Thermodynamics.dll")\n'
    'clr.AddReference(dwsimpath + "DWSIM.Thermodynamics.ThermoC.dll")\n'
    'clr.AddReference(dwsimpath + "DWSIM.UnitOperations.dll")\n'
    'clr.AddReference(dwsimpath + "DWSIM.Inspector.dll")\n'
    'clr.AddReference(dwsimpath + "System.Buffers.dll")'
)


class TimeoutException(Exception):
    """Exception raised for signaling a timeout."""
    pass


def signal_handler(signum, frame):
    """Signal handler for raising a TimeoutException."""
    raise TimeoutException("timeout")


class LLMClient:
    """Client for interacting with various language models."""
    logger.debug("Initializing LLMClient")

    def __init__(self, config):
        """
        Initialize the LLMClient with the given configuration.
        Args:
            config: Configuration object containing model and API key information.
        """

        if "gpt" in config.args.model:
            client = OpenAI(api_key=config.args.api_key)
            print("gpt client created...")
        elif "deepseek" in config.args.model:
            client = OpenAI(api_key=config.args.api_key, base_url="https://api.deepseek.com/v1")
            print("deepseek client created...")
        else:
            url = config.args.base_url
            if url:
                client = ollama.Client(host=url)
            else:
                client = ollama.Client()
            print("Ollama client created...")

        self.config = config
        self.model = self.config.args.model
        self.temperature = self.config.args.temperature
        self.client = client or openai

    def extract_code(self, generated_content):
        """
        Extract code from the generated content.

        Args:
            generated_content (str): The content generated by the LLM.

        Returns:
            tuple: A tuple containing a status code and the extracted code.
        """
        if not generated_content:
            raise ValueError("Empty generated content")
        regex = r".*?```.*?\n(.*?)```"
        match = re.search(regex, generated_content, re.DOTALL)
        if match:
            code = "\n".join(line for line in match.group(1).splitlines() if line.strip())
        else:
            return 1, ""
        if "dwsimpath" not in code:
            code = IMPORT_DWSIM + "\n" + code
        return 0, code

    def count_tokens(self, messages, model="gpt-4"):
        """
        Count the total number of tokens in a message history.

        Args:
            messages (list): List of messages (system, user, assistant).
            model (str): Model name (to use the correct tokenizer).

        Returns:
            int: Total token count.
        """
        tokenizer = tiktoken.encoding_for_model(model)
        total_tokens = sum(len(tokenizer.encode(msg["content"])) for msg in messages)
        return total_tokens

    def truncate_messages(self, model, msgs, max_tokens):
        """
        Truncate messages if they exceed the maximum token limit.

        Args:
            model (str): Model name.
            msgs (list): List of messages.
            max_tokens (int): Maximum number of tokens allowed.

        Returns:
            list: Truncated list of messages.
        """
        while self.count_tokens(msgs, model) > max_tokens:
            if len(msgs) > 1:
                msgs.pop(1)
        return msgs

    def call_llm(self, messages):
        """
        Call the language model with the given messages.

        Args:
            messages (list): List of messages to send to the LLM.

        Returns:
            str: The response from the LLM.
        """
        # Truncate messages if they exceed the maximum context length
        MAX_TOKENS = 16385
        messages = self.truncate_messages(self.model, messages, MAX_TOKENS)
        # print("MSG len ", len(messages))
        if any(model in self.model for model in OPENSOURCE_MODELS):
            print("Using Ollama for completion")
            signal.signal(signal.SIGALRM, signal_handler)
            signal.alarm(360)
            try:
                completion = ollama.chat(
                    model=self.model,
                    messages=messages,
                    options={"temperature": self.temperature, "top_p": 1.0}
                )
                signal.alarm(0)
            except TimeoutException:
                signal.alarm(0)
                self.restart_ollama()
                time.sleep(120)
            answer = completion.choices[0].message.content
        else:
            try:
                completion = self.client.chat.completions.create(
                    model=self.model,
                    messages=messages,
                    temperature=self.temperature
                )
                answer = completion.choices[0].message.content
            except openai.APIStatusError as e:
                print("APIStatusError encountered:", e)
                time.sleep(30)
                answer = ""

        return answer

    def get_model_dir(self):
        """
        Get the directory name for the model based on its identifier.

        Returns:
            str: The directory name for the model.
        """
        if "ft:gpt-3.5" in self.model:
            if "a:9HyyBpNI" in self.model:
                return "gpt3p5-ft-A"
            elif "b:9Hzb5l4S" in self.model:
                return "gpt3p5-ft-B"
            elif "c:9I0X557K" in self.model:
                return "gpt3p5-ft-C"
            else:
                return "unknown"
        elif "gpt-3" in self.model:
            return "gpt3p5"
        elif "gpt-4-turbo" in self.model:
            return "gpt4"
        elif "gpt-4o" in self.model:
            return "gpt4o"
        elif "deepseek-chat" in self.model:
            return "deepseek2"
        elif "deepseek-reasoner" in self.model:
            return "deepseek1"
        elif any(model in self.model for model in OPENSOURCE_MODELS):
            return self.model.replace(":", "-")
        return "unknown"

    def restart_ollama(self):
        """
        Restart the Ollama service.
        """
        from code_runner import kill_tmux_session, start_tmux_session
        kill_tmux_session("ollama")
        subprocess.run(['ollama', 'restart'], capture_output=True, text=True)
        start_tmux_session("ollama", "ollama serve")

    def summarize(self, text):
        """
        Summarize the given text using the LLM.

        Args:
            text (str): The text to be summarized.

        Returns:
            str: The summarized text.
        """
        messages = [
            {"role": "system", "content": "You are a helpful assistant that summarizes text."},
            {"role": "user", "content": f"Summarize the following text:\n\n{text}"}
        ]

        response = self.call_llm(messages)
        print("Summarized text: Original text size: ", len(text), "Summarized text size ", len(response))
        return response
